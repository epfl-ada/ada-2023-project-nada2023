{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the tsv files in MovieSummaries\n",
    "load_movies = pd.read_csv('MovieSummaries/movie.metadata.tsv', sep = '\\t', header = None)\n",
    "load_character = pd.read_csv('MovieSummaries/character.metadata.tsv', sep = '\\t', header = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = load_movies.copy()\n",
    "character = load_character.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the name of the colum of movie: wikip_ID, freebase_ID, name, release date, box office, runtime, languages, countries, genres\t\n",
    "movies.columns = ['Wiki_ID', 'Freebase_ID', 'name', 'release_date', 'box_office', 'runtime', 'languages', 'countries', 'genres']\n",
    "# Make the name of the columns of character wiki_ID, freebase_ID, character_name, actor_DOB, actor_gender, actor_height, actor_ethnicity, actor_name, actor_age, freebase_character_map\n",
    "character.columns = ['Wiki_ID', 'Freebase_ID','release_date', 'character_name', 'actor_DOB', 'actor_gender', 'actor_height', 'actor_ethnicity', 'actor_name', 'actor_age', '10', '11', '12']\n",
    "character = character.drop(['10', '11', '12'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>Wiki_ID</th>\n",
       "      <th>Freebase_ID</th>\n",
       "      <th>country</th>\n",
       "      <th>movie</th>\n",
       "      <th>gross</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0228333</td>\n",
       "      <td>975900</td>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>ghosts of mars</td>\n",
       "      <td>8709640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0228333</td>\n",
       "      <td>975900</td>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>France</td>\n",
       "      <td>ghosts of mars</td>\n",
       "      <td>1937390.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0228333</td>\n",
       "      <td>975900</td>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>Spain</td>\n",
       "      <td>ghosts of mars</td>\n",
       "      <td>509536.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0228333</td>\n",
       "      <td>975900</td>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>Germany</td>\n",
       "      <td>ghosts of mars</td>\n",
       "      <td>496679.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0228333</td>\n",
       "      <td>975900</td>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>ghosts of mars</td>\n",
       "      <td>148898.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90661</th>\n",
       "      <td>tt0120202</td>\n",
       "      <td>1918494</td>\n",
       "      <td>/m/0660qx</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>state and main</td>\n",
       "      <td>13732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90662</th>\n",
       "      <td>tt0120202</td>\n",
       "      <td>1918494</td>\n",
       "      <td>/m/0660qx</td>\n",
       "      <td>Australia</td>\n",
       "      <td>state and main</td>\n",
       "      <td>385631.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90663</th>\n",
       "      <td>tt0107057</td>\n",
       "      <td>664006</td>\n",
       "      <td>/m/030xw6</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>guilty as sin</td>\n",
       "      <td>22866222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90664</th>\n",
       "      <td>tt1606259</td>\n",
       "      <td>34980460</td>\n",
       "      <td>/m/0g4pl34</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>knuckle</td>\n",
       "      <td>2647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90665</th>\n",
       "      <td>tt1606259</td>\n",
       "      <td>34980460</td>\n",
       "      <td>/m/0g4pl34</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>knuckle</td>\n",
       "      <td>22008.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90666 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          tconst   Wiki_ID Freebase_ID         country           movie  \\\n",
       "0      tt0228333    975900   /m/03vyhn        Domestic  ghosts of mars   \n",
       "1      tt0228333    975900   /m/03vyhn          France  ghosts of mars   \n",
       "2      tt0228333    975900   /m/03vyhn           Spain  ghosts of mars   \n",
       "3      tt0228333    975900   /m/03vyhn         Germany  ghosts of mars   \n",
       "4      tt0228333    975900   /m/03vyhn  United Kingdom  ghosts of mars   \n",
       "...          ...       ...         ...             ...             ...   \n",
       "90661  tt0120202   1918494   /m/0660qx         Denmark  state and main   \n",
       "90662  tt0120202   1918494   /m/0660qx       Australia  state and main   \n",
       "90663  tt0107057    664006   /m/030xw6        Domestic   guilty as sin   \n",
       "90664  tt1606259  34980460  /m/0g4pl34        Domestic         knuckle   \n",
       "90665  tt1606259  34980460  /m/0g4pl34  United Kingdom         knuckle   \n",
       "\n",
       "            gross  \n",
       "0       8709640.0  \n",
       "1       1937390.0  \n",
       "2        509536.0  \n",
       "3        496679.0  \n",
       "4        148898.0  \n",
       "...           ...  \n",
       "90661     13732.0  \n",
       "90662    385631.0  \n",
       "90663  22866222.0  \n",
       "90664      2647.0  \n",
       "90665     22008.0  \n",
       "\n",
       "[90666 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## read grossed merged clean csv\n",
    "gross = pd.read_csv('gross_merged_clean.csv')\n",
    "gross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "For The first question we will need the gross of the movies in the different region of the world, as well as their release year and their genre.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing 'box_office' values before imputation: 89.72%\n",
      "Average coherence deviation: 0.34\n"
     ]
    }
   ],
   "source": [
    "movies_clean_gross = movies.copy()\n",
    "gross_clean = gross.copy()\n",
    "\n",
    "# Sum up the gross for each movie in the `gross` dataframe\n",
    "gross_sum = gross_clean.groupby('Wiki_ID')['gross'].sum().reset_index()\n",
    "\n",
    "# Merge this sum with the `movies` dataframe\n",
    "movies_with_gross = movies_clean_gross.merge(gross_sum, on='Wiki_ID', how='left')\n",
    "\n",
    "# Filter out rows where 'box_office' is not missing\n",
    "non_missing_box_office = movies_with_gross.dropna(subset=['box_office'])\n",
    "\n",
    "\n",
    "missing_percentage_before = movies_with_gross['box_office'].isna().sum() / len(movies_with_gross) * 100\n",
    "print(f\"Percentage of missing 'box_office' values before imputation: {missing_percentage_before:.2f}%\")\n",
    "\n",
    "# Check coherence for non-missing 'box_office' values\n",
    "coherence_check = ((non_missing_box_office['box_office'] - non_missing_box_office['gross']).abs() / non_missing_box_office['box_office']).mean()\n",
    "print(f\"Average coherence deviation: {coherence_check:.2f}\")\n",
    "\n",
    "# movies_with_gross['box_office'].fillna(movies_with_gross['gross'], inplace=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of box_office missing values, and for the ones we have, the summed gross differ from it by 34%, so replacing the Nans by the summed gross will not be accurate.\n",
    "\n",
    "Instead, we should use the summed gross as the global box office for all the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The release date of the movie is in the format of YYYY-MM-DD, we only need the year \n",
    "# so we split the date and only keep the year\n",
    "movies_clean_release = movies.copy()\n",
    "movies_clean_release['release_date'] = movies_clean_release['release_date'].str[:4]\n",
    "movies_clean_release['release_date'] = pd.to_numeric(movies_clean_release['release_date'], errors='coerce')\n",
    "# Here the errors ignore is to keep the nan in the release date column\n",
    "movies_clean_release['release_date'] = movies_clean_release['release_date'].astype(np.int64, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_clean_genre = movies_clean_release.copy()\n",
    "movies_clean_genre['genres'] = movies_clean_genre['genres'].apply(lambda x: list(ast.literal_eval(x).values()))\n",
    "# This line is to make the empty list to be nan so the nan are uniform\n",
    "movies_clean_genre['genres'] = movies_clean_genre['genres'].apply(lambda x: x if len(x) != 0 else np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_genre_release = gross.copy()\n",
    "#merge gross and movies_clean_genre on wiki_ID\n",
    "gross_genre_release = gross_genre_release.merge(movies_clean_genre[['Wiki_ID', 'release_date', 'genres']], on = 'Wiki_ID', how = 'left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of missing values in the genre column is 0.14%\n"
     ]
    }
   ],
   "source": [
    "# count the numers of nan in genre\n",
    "missing_genre_value = gross_genre_release.drop_duplicates(subset='Wiki_ID')['genres'].isna().sum()/gross_genre_release.drop_duplicates(subset='Wiki_ID')['genres'].shape[0]*100\n",
    "print('The percentage of missing values in the genre column is {:.2f}%'.format(missing_genre_value))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a very low percentage of genre missing we can therefore have a good analysis about this metric. We don't have any missing values for the release date because it was used as a key when merging the IMdB dataset with the CMU datset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_clean_country = movies_clean_genre.copy()\n",
    "movies_clean_country['countries'] = movies_clean_country['countries'].apply(lambda x: list(ast.literal_eval(x).values()))\n",
    "# This line is to make the empty list to be nan so the nan are uniform\n",
    "movies_clean_country['countries'] = movies_clean_country['countries'].apply(lambda x: x if len(x) != 0 else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_country = gross.copy()\n",
    "#merge gross and movies_clean_genre on wiki_ID\n",
    "gross_country = gross_country.merge(movies_clean_country[['Wiki_ID', 'countries']], on = 'Wiki_ID', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of missing values in the countries column is 1.70%\n"
     ]
    }
   ],
   "source": [
    "# count the numers of nan in countries\n",
    "missing_country_value = gross_country.drop_duplicates(subset='Wiki_ID')['countries'].isna().sum()/gross_country.drop_duplicates(subset='Wiki_ID')['countries'].shape[0]*100\n",
    "print('The percentage of missing values in the countries column is {:.2f}%'.format(missing_country_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of movies that are co-produced by more than one country is 24.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4834/30425266.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  coprood_count = coprood_count.drop_duplicates(subset='Wiki_ID')[coprood_count['countries'].apply(lambda x: len(x)) > 1].shape[0]/gross_country.drop_duplicates(subset='Wiki_ID')['countries'].shape[0]*100\n"
     ]
    }
   ],
   "source": [
    "coprood_count = gross_country.dropna(subset='countries')\n",
    "coprood_count = coprood_count.drop_duplicates(subset='Wiki_ID')[coprood_count['countries'].apply(lambda x: len(x)) > 1].shape[0]/gross_country.drop_duplicates(subset='Wiki_ID')['countries'].shape[0]*100\n",
    "\n",
    "print('The number of movies that are co-produced by more than one country is {:.2f}%'.format(coprood_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a very low percentage of countries missing we can therefore have a good analysis about this metric. And the level of co-production is relatively high so it will still make sense to do our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the gender representation in the moviee. We decided to only use the values that were given for each of the movies, making the assumption that if the gender of an actor is not given the character was probably a small part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiki_ID\n",
      "330         1.000000\n",
      "3217        0.230769\n",
      "3333        0.428571\n",
      "3746        0.214286\n",
      "3837        0.100000\n",
      "              ...   \n",
      "37373877    0.500000\n",
      "37476824    0.400000\n",
      "37478048    0.250000\n",
      "37492363         NaN\n",
      "37501922    0.500000\n",
      "Length: 64330, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# go in the character dataset and find the percentage of gender F in each movie\n",
    "character_gender = character.copy()\n",
    "\n",
    "#count the number of \"F\" for each wiki_ID in character_gender\n",
    "# groupby wiki_ID and count the number of \"F\" in each group\n",
    "character_groupby = character_gender.groupby('Wiki_ID')\n",
    "character_groupby_tot = character_groupby[\"actor_gender\"].count()\n",
    "character_groupby_F = character_groupby.apply(lambda x: (x['actor_gender'] == 'F').sum())\n",
    "ratio_female_to_total = character_groupby_F / character_groupby_tot\n",
    "print(ratio_female_to_total)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries in character dataframe: 450669\n",
      "Number of missing values in 'actor_ethnicity': 344611\n",
      "Percentage of missing values in 'actor_ethnicity': 76.47%\n"
     ]
    }
   ],
   "source": [
    "# Analyzing Ethnicity Data\n",
    "print(f\"Total entries in character dataframe: {len(character)}\")\n",
    "\n",
    "# Check for missing values in 'actor_ethnicity'\n",
    "missing_ethnicity = character['actor_ethnicity'].isna().sum()\n",
    "print(f\"Number of missing values in 'actor_ethnicity': {missing_ethnicity}\")\n",
    "print(f\"Percentage of missing values in 'actor_ethnicity': {missing_ethnicity / len(character) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor_ethnicity\n",
      "Unknown       76.466542\n",
      "/m/0dryh9k     3.628162\n",
      "/m/0x67        2.254204\n",
      "/m/041rx       2.245772\n",
      "/m/02w7gg      1.367966\n",
      "                ...    \n",
      "/m/043_z22     0.000222\n",
      "/m/03x_fq7     0.000222\n",
      "/m/01hphz      0.000222\n",
      "/m/033fjj      0.000222\n",
      "/m/013y54      0.000222\n",
      "Name: proportion, Length: 480, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Filling Nan with 'Unknown' ethnicity\n",
    "character_ethnicity = character_gender.copy()\n",
    "character_ethnicity['actor_ethnicity'] = character_ethnicity['actor_ethnicity'].fillna('Unknown')\n",
    "\n",
    "# Analyzing ethnicity distribution\n",
    "ethnicity_distribution = character['actor_ethnicity'].value_counts(normalize=True) * 100\n",
    "print(ethnicity_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English           40891\n",
      "Hindi              3744\n",
      "Spanish            3673\n",
      "French             3428\n",
      "Silent             3183\n",
      "                  ...  \n",
      "Sumerian              1\n",
      "Tamang                1\n",
      "Judeo-Georgian        1\n",
      "Cheyenne              1\n",
      "Kuna                  1\n",
      "Name: count, Length: 197, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Function to extract languages from the dictionary string\n",
    "def extract_languages(lang_str):\n",
    "    if pd.isna(lang_str):\n",
    "        return ['Unknown']\n",
    "    lang_dict = ast.literal_eval(lang_str)\n",
    "    languages =  [lang.split(' ')[0] for lang in lang_dict.values()]\n",
    "    \n",
    "    return [lang for lang in languages if lang != 'Silent']\n",
    "\n",
    "\n",
    "movies_clean_languages = movies_clean_country.copy()\n",
    "movies_clean_languages['languages'] = movies_clean_languages['languages'].apply(extract_languages)\n",
    "\n",
    "# Counting languages\n",
    "language_counts = pd.Series([lang for sublist in movies['languages'] for lang in sublist]).value_counts()\n",
    "print(language_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of multilingual movies: 7207 over 81741 movies.\n",
      "49                             [Afrikaans, English]\n",
      "56                            [Cantonese, Standard]\n",
      "58    [English, Swedish, German, Norwegian, Danish]\n",
      "65                               [English, Spanish]\n",
      "95                               [Italian, English]\n",
      "Name: languages, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Count the number of movies with more than one language\n",
    "multilingual_movies = movies_clean_languages['languages'].apply(lambda x: len(x) > 1)\n",
    "\n",
    "print(f\"Number of multilingual movies: {multilingual_movies.sum()} over {len(multilingual_movies)} movies.\")\n",
    "print(movies_clean_languages['languages'][multilingual_movies].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common multilingual language combinations:\n",
      "multilingual_combination\n",
      "English-French        522\n",
      "English-Spanish       506\n",
      "English-Italian       278\n",
      "English-German        266\n",
      "Cantonese-Standard    246\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Creating a new dataframe for multilingual movies for language combination analysis\n",
    "multilingual_combinations_df = movies_clean_languages.copy()\n",
    "multilingual_combinations_df = multilingual_combinations_df[multilingual_combinations_df['languages'].apply(lambda x: len(x) > 1 and 'Unknown' not in x)]\n",
    "\n",
    "# Create a distinct identifier for each multilingual language combination\n",
    "multilingual_combinations_df['multilingual_combination'] = multilingual_combinations_df['languages'].apply(lambda x: '-'.join(sorted(x)))\n",
    "\n",
    "# Count the occurrences of each multilingual language combination\n",
    "multilingual_combination_counts = multilingual_combinations_df['multilingual_combination'].value_counts()\n",
    "print(\"Most common multilingual language combinations:\")\n",
    "print(multilingual_combination_counts.head())  # Adjust the number of combinations to display as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wiki_ID</th>\n",
       "      <th>Freebase_ID</th>\n",
       "      <th>release_date</th>\n",
       "      <th>character_name</th>\n",
       "      <th>actor_DOB</th>\n",
       "      <th>actor_gender</th>\n",
       "      <th>actor_height</th>\n",
       "      <th>actor_ethnicity</th>\n",
       "      <th>actor_name</th>\n",
       "      <th>actor_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>429940</th>\n",
       "      <td>37501922</td>\n",
       "      <td>/m/0c0m5vt</td>\n",
       "      <td>1992</td>\n",
       "      <td>John Hunter</td>\n",
       "      <td>1966-10-11</td>\n",
       "      <td>M</td>\n",
       "      <td>1.765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Luke Perry</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429941</th>\n",
       "      <td>37501922</td>\n",
       "      <td>/m/0c0m5vt</td>\n",
       "      <td>1992</td>\n",
       "      <td>Craig Murphy</td>\n",
       "      <td>1969-07-28</td>\n",
       "      <td>F</td>\n",
       "      <td>1.720</td>\n",
       "      <td>/m/041rx</td>\n",
       "      <td>Alexis Arquette</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Wiki_ID Freebase_ID release_date character_name   actor_DOB  \\\n",
       "429940  37501922  /m/0c0m5vt         1992    John Hunter  1966-10-11   \n",
       "429941  37501922  /m/0c0m5vt         1992   Craig Murphy  1969-07-28   \n",
       "\n",
       "       actor_gender  actor_height actor_ethnicity       actor_name  actor_age  \n",
       "429940            M         1.765             NaN       Luke Perry       25.0  \n",
       "429941            F         1.720        /m/041rx  Alexis Arquette       22.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character[character['Wiki_ID']==37501922    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
